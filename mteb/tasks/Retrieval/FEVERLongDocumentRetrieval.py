import datasets
from ...abstasks.AbsTaskRetrieval import AbsTaskRetrieval


class FEVERLongDocumentRetrieval(AbsTaskRetrieval):

    _EVAL_SPLIT = 'test'

    @property
    def description(self):
        return {
            'name': 'FEVERLongDocumentRetrieval',
            'hf_hub_name': 'fever',
            "description": (
                "FEVER (Fact Extraction and VERification) consists of 18,544 claims generated by altering sentences"
                " extracted from Wikipedia and subsequently verified without knowledge of the sentence they were"
                " derived from. This task takes claims which have supporting evidence as queries."
                " Whole Wikipedia articles are retrieved as documents. The task is limited to 1M documents and"
                " 4255 queries (claims)."
            ),
            "reference": "https://fever.ai/",
            "type": "Retrieval",
            "category": "s2p",
            "eval_splits": ["test"],
            "eval_langs": ["en"],
            "main_score": "ndcg_at_10",
        }

    def load_data(self, **kwargs):
        if self.data_loaded:
            return

        query_rows = datasets.load_dataset(self.description['hf_hub_name'], 'v1.0', split='paper_test')
        corpus_rows = datasets.load_dataset('wikipedia', '20220301.en', split='train')
        corpus_titles = set(title.replace(' ', '_') for title in corpus_rows['title'])
        filtered_queries = {
            str(row['id']): {'evidence_wiki_url': row['evidence_wiki_url'], 'claim': row['claim']}
            for row in query_rows
            if row['label'] == 'SUPPORTS' and row['evidence_wiki_url'] in corpus_titles
            and row['evidence_sentence_id'] > 10
        }
        q_evidence_wiki_urls = set(r['evidence_wiki_url'] for r in list(filtered_queries.values()))
        # extend q_evidence_wiki_urls with 1M more wiki urls from corpus_rows
        q_evidence_wiki_urls.update(t.replace(' ', '_') for t in set(corpus_rows[:1_000_000]['title']))

        self.queries = {
            self._EVAL_SPLIT: {str(row['id']): row['claim'] for row in query_rows if str(row['id']) in filtered_queries}
        }
        self.corpus = {
            self._EVAL_SPLIT: {
                row['title'].replace(' ', '_'): {'text': row['text']}
                for row in corpus_rows if row['title'].replace(' ', '_') in q_evidence_wiki_urls
            }
        }

        self.relevant_docs = {self._EVAL_SPLIT: {key: {values['evidence_wiki_url']: 1} for key, values in filtered_queries.items()}}
        print(f'Loaded {len(self.queries[self._EVAL_SPLIT])} queries and {len(self.corpus[self._EVAL_SPLIT])} documents.')
        self.data_loaded = True
